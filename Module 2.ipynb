{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1f4cd4-2647-449a-8947-3c7394236027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "New best model saved with val_accuracy: 0.9956 (Fold 1)\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step\n",
      "Fold 2\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step\n",
      "Fold 3\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n",
      "Fold 4\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from keras.regularizers import l2\n",
    "from keras.layers import Input\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "# Define Focal Loss\n",
    "def focal_loss(gamma=2., alpha=0.25):\n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        y_pred = tf.clip_by_value(y_pred, tf.keras.backend.epsilon(), 1 - tf.keras.backend.epsilon())\n",
    "        loss = - alpha * tf.pow((1 - y_pred), gamma) * tf.math.log(y_pred)\n",
    "        return tf.keras.backend.mean(tf.keras.backend.sum(loss * y_true, axis=1))\n",
    "    return focal_loss_fixed\n",
    "\n",
    "# Define the model architecture\n",
    "def create_model(input_shape, num_classes):\n",
    "    model = Sequential([\n",
    "        Input(shape=input_shape),  # Specify the input shape here\n",
    "        Conv1D(64, kernel_size=3, activation='relu', kernel_regularizer=l2(0.01), padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling1D(pool_size=1),\n",
    "        Dropout(0.3),\n",
    "\n",
    "        Conv1D(128, kernel_size=3, activation='relu', kernel_regularizer=l2(0.01), padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling1D(pool_size=1),\n",
    "        Dropout(0.3),\n",
    "\n",
    "        Conv1D(256, kernel_size=3, activation='relu', kernel_regularizer=l2(0.01), padding='same'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "\n",
    "        Flatten(),\n",
    "        Dense(128, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.4),\n",
    "\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "                  loss=focal_loss(alpha=.25, gamma=2),\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Load your dataset\n",
    "file_path = 'cleaned_second_module_input1.csv'  # Update with your file path\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Prepare the data\n",
    "features = [f'votes_{i}' for i in range(1, 11)] + [f'rating_{i}' for i in range(1, 11)] + ['calculated_score']\n",
    "X = data[features].values\n",
    "y = data['predicted_popularity_class'].values\n",
    "\n",
    "# Encode the target labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# One-hot encode the labels\n",
    "y_encoded = to_categorical(y_encoded)\n",
    "\n",
    "# Stratified K-Fold Cross-validation\n",
    "n_splits = 5\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Lists to store results\n",
    "histories = []\n",
    "y_true_all = []\n",
    "y_pred_all = []\n",
    "best_val_accuracy = 0  # Track the best validation accuracy\n",
    "best_model_filename = 'best_model.keras'  # Filename to save the best model\n",
    "\n",
    "for fold, (train_index, val_index) in enumerate(skf.split(X, np.argmax(y_encoded, axis=1)), 1):\n",
    "    print(f\"Fold {fold}\")\n",
    "    \n",
    "    X_train, X_val = X[train_index], X[val_index]\n",
    "    y_train, y_val = y_encoded[train_index], y_encoded[val_index]\n",
    "    \n",
    "    # Apply SMOTE\n",
    "    smote = SMOTE(random_state=42, k_neighbors=2)\n",
    "    X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_resampled = scaler.fit_transform(X_train_resampled)\n",
    "    X_val = scaler.transform(X_val)\n",
    "    \n",
    "    X_train_resampled = X_train_resampled.reshape((X_train_resampled.shape[0], X_train_resampled.shape[1], 1))\n",
    "    X_val = X_val.reshape((X_val.shape[0], X_val.shape[1], 1))\n",
    "    \n",
    "    # Create and train the model\n",
    "    model = create_model((21, 1), y_encoded.shape[1])  # Number of classes in one-hot encoded labels\n",
    "    \n",
    "    callbacks = [\n",
    "        EarlyStopping(patience=10, restore_best_weights=True),\n",
    "        ReduceLROnPlateau(factor=0.2, patience=5, min_lr=0.00001),\n",
    "        ModelCheckpoint(best_model_filename, monitor='val_accuracy', save_best_only=True, mode='max', verbose=0)\n",
    "    ]\n",
    "    \n",
    "    history = model.fit(X_train_resampled, y_train_resampled,\n",
    "                        epochs=100,\n",
    "                        batch_size=32,\n",
    "                        validation_data=(X_val, y_val),\n",
    "                        callbacks=callbacks,\n",
    "                        verbose=0)\n",
    "    \n",
    "    # Evaluate the model on the validation set\n",
    "    val_accuracy = model.evaluate(X_val, y_val, verbose=0)[1]  # [1] is accuracy\n",
    "\n",
    "    # Check if this fold produced the best validation accuracy\n",
    "    if val_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = val_accuracy\n",
    "        model.save(best_model_filename)  # Save the best model to file\n",
    "        print(f\"New best model saved with val_accuracy: {best_val_accuracy:.4f} (Fold {fold})\")\n",
    "\n",
    "    histories.append(history)\n",
    "    \n",
    "    # Predictions for analysis\n",
    "    y_pred = model.predict(X_val)\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "    \n",
    "    y_true_all.extend(np.argmax(y_val, axis=1))  # Convert one-hot to integers for true labels\n",
    "    y_pred_all.extend(y_pred_classes)\n",
    "\n",
    "# Print overall results\n",
    "print(\"Classification Report for Best Model Across All Folds:\")\n",
    "print(classification_report(y_true_all, y_pred_all, target_names=label_encoder.classes_))\n",
    "print(confusion_matrix(y_true_all, y_pred_all))\n",
    "\n",
    "# Load the best model for further use\n",
    "from keras.models import load_model\n",
    "best_model = load_model(best_model_filename, custom_objects={'focal_loss_fixed': focal_loss()})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b0b3a8cf-eaf8-418e-ba91-c74ca7715278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "Predicted Label: F\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import load_model\n",
    "import tensorflow as tf\n",
    "import joblib\n",
    "\n",
    "# Define Focal Loss (used during model training)\n",
    "def focal_loss(gamma=2., alpha=0.25):\n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        y_pred = tf.clip_by_value(y_pred, tf.keras.backend.epsilon(), 1 - tf.keras.backend.epsilon())\n",
    "        loss = - alpha * tf.pow((1 - y_pred), gamma) * tf.math.log(y_pred)\n",
    "        return tf.keras.backend.mean(tf.keras.backend.sum(loss * y_true, axis=1))\n",
    "    return focal_loss_fixed\n",
    "\n",
    "# Load the scaler\n",
    "scaler = joblib.load('scaler.joblib')  # Load the scaler to scale inputs properly\n",
    "\n",
    "# Input example\n",
    "input_example = [2911.0, 141.0, 14021.0, 3220.0, 25927.0, \n",
    "                 2156.0, 1354.0, 172.0, 376.0, 798.0, \n",
    "                 1.0, 1.2, 1.5, 1.2, 1.8, 1.7, 1.4, 1.2, 1.7, 1.4, \n",
    "                 0.15]\n",
    "\n",
    "# Step 1: Scale the input\n",
    "input_array = np.array(input_example).reshape(1, -1)  # Reshape for scaling\n",
    "input_scaled = scaler.transform(input_array)  # Use the loaded scaler to scale the input\n",
    "\n",
    "# Step 2: Reshape the input to fit the CNN model (samples, features, 1)\n",
    "input_scaled_reshaped = input_scaled.reshape((input_scaled.shape[0], input_scaled.shape[1], 1))\n",
    "\n",
    "# Step 3: Load the trained model and specify the custom loss function\n",
    "model = load_model('best_model.keras', custom_objects={'focal_loss_fixed': focal_loss()})\n",
    "\n",
    "# Step 4: Predict the class\n",
    "predicted_probabilities = model.predict(input_scaled_reshaped)\n",
    "predicted_class = np.argmax(predicted_probabilities, axis=1)  # Get the class with the highest probability\n",
    "\n",
    "# Step 5: Manually define the labels (in the same order as the classification report/confusion matrix)\n",
    "labels = [\"A\", \"AA\", \"F\", \"H\", \"SDH\", \"SH\"]\n",
    "\n",
    "# Step 6: Map the predicted class to its corresponding label\n",
    "predicted_label = labels[predicted_class[0]]\n",
    "\n",
    "print(f\"Predicted Label: {predicted_label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264da8a4-a4f3-49b7-b926-c70a9f0d2e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_example = [2917451.0, 1600541.0, 1464021.0, 322800.0, 2590527.0, 2154266.0, 1340354.0, 179212.0, 37654.0, 78938.0,9.0, 8.2, 8.5, 7.2, 8.8, 8.7, 8.4, 6.2, 6.7, 6.4, 0.95]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
